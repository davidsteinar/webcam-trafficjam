{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Dataset Parameters - CHANGE HERE\n",
    "DATASET_PATH = '../data/TRANCOS_v3/'\n",
    "\n",
    "# Image Parameters\n",
    "N_CLASSES = 1\n",
    "IMG_HEIGHT = 112\n",
    "IMG_WIDTH = 200\n",
    "CHANNELS = 3\n",
    "\n",
    "\n",
    "# Reading the dataset\n",
    "def read_images(dataset_path, batch_size, mode):\n",
    "    imagepaths, labels = [], []\n",
    "        # Read dataset file\n",
    "    if mode == 'train':\n",
    "        dataset_path_mod = dataset_path + 'image_sets/training.txt'\n",
    "    elif mode == 'validation':\n",
    "        dataset_path_mod = dataset_path + 'image_sets/validation.txt'\n",
    "    elif mode == 'test':\n",
    "        dataset_path_mod = dataset_path + 'image_sets/test.txt'\n",
    "        \n",
    "    with open(dataset_path_mod, 'r') as f:\n",
    "        data = f.read().splitlines()\n",
    "        \n",
    "    for d in data:\n",
    "        imagepaths.append(dataset_path+'images/'+d)\n",
    "        \n",
    "        label = (dataset_path+'images/'+d).replace('.jpg','.txt')\n",
    "        with open(label, 'r', encoding='utf-8') as f:\n",
    "            for i, l in enumerate(f):\n",
    "                pass\n",
    "            \n",
    "        labels.append(i+1)\n",
    "\n",
    "    # Convert to Tensor\n",
    "    imagepaths = tf.convert_to_tensor(imagepaths, dtype=tf.string)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    # Build a TF Queue, shuffle data\n",
    "    image, label = tf.train.slice_input_producer([imagepaths, labels],\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    # Read images from disk\n",
    "    image = tf.read_file(image)\n",
    "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
    "    # Resize images to a common size\n",
    "    image = tf.image.resize_images(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "    # Normalize\n",
    "    #image = image * 1.0/127.5 - 1.0 ################################################################### sketchy\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    # Create batches\n",
    "    X, Y = tf.train.batch([image, label], batch_size=batch_size,\n",
    "                          capacity=batch_size * 8)#,\n",
    "                          #num_threads=4) ############################################################# num threads?\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 1\n",
    "batch_size = 10\n",
    "display_step = 100\n",
    "dropout = 0.75\n",
    "\n",
    "# Build the data input\n",
    "X_train_batch, Y_train_batch = read_images(DATASET_PATH, batch_size, mode='train')\n",
    "X_test_batch, Y_test_batch   = read_images(DATASET_PATH, batch_size, mode='test')\n",
    "X_val_batch, Y_val_batch     = read_images(DATASET_PATH, batch_size, mode='validation')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, reuse=False):\n",
    "    # Define a scope for reusing the variables\n",
    "\n",
    "    conv1    = tf.layers.conv2d(inputs=x, \n",
    "                                filters=32, \n",
    "                                kernel_size=5,\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "    maxpool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                       pool_size=2,\n",
    "                                       strides=2)\n",
    "\n",
    "    conv2    = tf.layers.conv2d(inputs=maxpool1, \n",
    "                                filters=32, \n",
    "                                kernel_size=5,\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "    maxpool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                       pool_size=2,\n",
    "                                       strides=2)\n",
    "\n",
    "    conv3    = tf.layers.conv2d(inputs=maxpool2, \n",
    "                                filters=32, \n",
    "                                kernel_size=3,\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "    maxpool3 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                       pool_size=2,\n",
    "                                       strides=2)\n",
    "\n",
    "    flat     = tf.contrib.layers.flatten(inputs=maxpool3)\n",
    "\n",
    "    dropout1 = tf.layers.dropout(inputs=flat, \n",
    "                                 rate=dropout, \n",
    "                                 training=True)\n",
    "\n",
    "    fullyconnected1   = tf.layers.dense(inputs=dropout1, \n",
    "                               units=500, \n",
    "                               activation=tf.nn.relu)\n",
    "\n",
    "    dropout2 = tf.layers.dropout(inputs=fullyconnected1, \n",
    "                                 rate=dropout, \n",
    "                                 training=True)\n",
    "    out = tf.layers.dense(dropout2, \n",
    "                          units=1)\n",
    "\n",
    "    #hidden = tf.layers.dense(inputs=x,\n",
    "    #                         units=100,\n",
    "    #                         activation=tf.nn.relu)\n",
    "    #out = tf.layers.dense(hidden, 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# Because Dropout have different behavior at training and prediction time, we\n",
    "# need to create 2 distinct computation graphs that share the same weights.\n",
    "\n",
    "# Create a graph for training\n",
    "logits_train = conv_net(X_train_batch)\n",
    "# Create another graph for testing that reuse the same weights\n",
    "#logits_test = conv_net(X)\n",
    "\n",
    "# Define loss and optimizer (with train logits, for dropout to take effect)\n",
    "loss_op   = tf.reduce_mean(tf.square(tf.subtract(Y_train_batch, logits_train)))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op  = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "#test_loss = tf.reduce_mean(tf.square(tf.subtract(Y, logits_test)))\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver object\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  \n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(10):\n",
    "        #print(sess.run(X_train_batch))\n",
    "        xinput = sess.run(X_train_batch)\n",
    "        yinput  = sess.run(Y_train_batch)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 112, 200, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAABwCAIAAADUo6jRAAAN7UlEQVR4nO1d27bjIAgFz/z/Hwfm\nAUW8xtzb1D2zetokGmN2ABEJLsvinENEmJjYCyJiZrvFAUC2aWJiK0rBNIk1cQnc0w2YeAPqEmti\n4iAYAFJuTWJNnIHCmnIAAHNEOHEYGYkcAOBk1sTZ+HeksPrA/LiSPW/FlCMiAHAualvdwsCWzUSE\niIhIRA4dA6sxKDUjIhEjAjE5TL1uDCKIYzOMLanFmfM6syO/GmSuyD1xRQzAUPix9mFZlr+/P7kx\n/tMhcLxVwpWkDHqeZaxSdjrniCkr5QnnpLq8TmICRGZm5rKs8smSSTZKkd2X/zkgZkD0/x9SPljo\nvf3E+vv7kxsjcgjkhrnuhTHo8VpKmcDkpUtZzjmn0k4L6q5QD/65PxF+I+13zllp+qVggEgmhn73\nX9yS5Ck9pApV0cgnMYkwUCFULSXbDSGclEKHtnhWSrbILqtJfdlwLpF5DocYUz3RV4DYmx1shFTW\n30RGPxrGWb0JDM4hEZnnGwB4a7cwZ7wCXJYFEb+0f38WVSVun2Q2hyGImYGQsQoAAFy0TREQgHOC\njqCcKzwksSYewUIMgGGcErcTADA7Y1OG7/4wYsaareEtTmB3nodgEuv78NexpHTkBMjALAxDVEnF\nYei+EKExSPjsAfI/acXE14HYWFfyJwyBSQaKiBTGvy5811ud6U0+zgIEa2ZNifWVENPJ30d10WEk\nGiJ4chUcCoPquJmZT5ctk1iHcMKDfuDU8qf0iCIAB2NLoUa6pxomx5+gB9NRwSTWITzCKiIC9LTh\nGrf7jp7x7UfwDyDn2sSHI/ENfeqU1HRfTVyCSayJSzCJNXEJJNDvQ/X0xPdiSqyJSzCJNXEJJrEm\nLoEDqCyxmJg4iCmxJi7BJNbEJZjEmrgEk1gTl2ASa+ISTGJNXIJJrIlLMIk1cQkmse5DmeDgRciv\na4YmXwi74hh9DMlbA0kw49bbJBYXP/mu0GtOv5tY9AeXXDyGt0ksuYF6J++8n9mpAWQB1s2t+BS8\njViC6p1kMFIFNanW0XPZ9EgAAIgcf/4ipQTvJFYVKLlZ9PtanoKQKKNzAKfL87JkYD8dmLtCLJsI\n7wXA5o8EUbZhzz5SVmHMK5gd8Z6uW8PGUeGbWJVh5cKELFxP6qM0CukCITGuXttnHbx9VHgK0K86\nhxZHJNNcsTeMQUMPvyQX5S5MYnXhcwHVwMGPkQwK/D7gUPA3qFU6fiexeihz6VaO6e5/sS3RxztH\nhTeMOZL6ETWXkKrBnzLdS7yTWJtZxT7j1G4qxCTkkP79ERQKf6pCAPCZykousPnsY+2YfL/k/Hzv\nnPQkFgDIba7mITafHdgDGlwpE8o2ZzHfwbV3qkJIc9Cv4rhBFnN7DihBVvphdWN8QcvXzArlbqz3\nEsu8OUd+bq5B5/ruvrWPeijOovL7VSHizuEZhv8XwlCo4dq4m2R7rzcv91qJVcUm/Xgdp7CtMFGC\nI/Q1ZoB3TteuzruvFY14klj3z3B3Thdf+3F+Mv1N4DBfFIBFGMVliJFkW+lVCNYnifVRXmmbW/+J\n89tIsQRdSp1v3Ht5iRvfqVMY7++3sY6AvSeiZ0+rn+KYU6pj0Ym7a8yLsQXVuG3J+b5JQFYmS3/N\nxuqjfPNqrpVqMD73/be5fDdscobd9Q7Aqj/9yY14oXFMYkUMqub+m3/3vRc4fzGsbBxoS+eoVTUZ\nOVQ0YrzlXnWmbyeFSawd6Hf6KYZjHufVHOXEjWLgWzI1A//TXYgYlLjIKdTaBptaVdIvt7EOTsYR\nkbzisR6xpxFZzERs32h6LqKzt23sCQ9w7JLzY/yFoFXrq7VoY6qkf7nEku7uPn+Zxkh+tl48G7WH\nfA7qUC25V6zZq2DmqkOsL2n6wRcx5ofX2xj6FoCZClV4msT65Cjc0NcjA6vOkR760vgwXhtvBqB9\nf3U6+b21/3Tsxmk9fbCPl04OtjE/NqZsDPWjT5NYH+WUamCkhWyG/TJbWJlmSV/XfdSVRMx9QdLH\nyNAVbMhZy+m/49yhwoUoezJergq3I7Vrq0ec9whd+jRmwYsrKhJ7P1dRmvAvN963YMBaHa5oXDdd\nBxO8uOrlhcSfhfKnFpzYqKPc/k0SK2v9eY+7VX/FOfV0/kjoKA1rnsNtk6GmX1peifiW3kCYxpFR\nyBW782vhUB/IuzlTfBOxLrtJxpjOVcbOU468ZTnztldZKEIitfgLbOmXzMYqhpadmsrhJwAgY3jJ\n648E+u1C/w61XUj22y5ve0vFbH2Wto4krIcM4lM1NMpR0cwY32CuOEoscyVfE0a7hnghwf6Fhq5k\nPf5gZMuO2aRqqaojY0Xg+YJbm594Tc4nFla/fjesBOq7VbMjr3m0tNYtFtud3h9EJCJOzf1zVeGB\nCMQvgxlD5RN2cFYPRPmUOvprhwIDa/TpMz7FK22s11Iq1XTlLBBAHElF333DvzqKDfxAAMCQ9Cbc\nYmZE3BdtcRzqx/rcCZlPAAIaQZ8MG9NPtMFzeHgWPAN3zHyQiaZgTzEDgMygn9iAccxR4SjMhGMm\nrqqzjdmPpvklsRPWVHWuLVcMS1izctWDAx/WHkqsF2qxbljmblRp1OLN+tpEtymy3By8wxOhAvQG\n2h2SWFn86m2LSfoohf/1HnCEaLkDGLOr6BDOSt0A4wXFc1VzBz1irY+es5nLW3pK1wbKI5iFTcID\nWoCD5R68AkNdsaGRXukh7hsOHJFzu9EjVssh2Dcx1vccQ5owqDqrdTMw7RhEkHkYMBvtwTk6kj4N\n0FFt9rgFtY490Q12dIT1Pes1PD75fyqwEJpoBoklUgd64shI6604zW9i1cEADS+xhsVL69lSK1Vr\n6tXX6u9XoNOX2UWzzt72O/8Ilcyyx20GAyIeSU+iqjD1IA+Pp9Iz7xzgvAtqyNsHrD4nYY5Qgg2S\nbQiJd7Ry5moZ78QHADs0E/GFeuPCwI2DwyyrBpdlQRTfyfqVtAL4k3ZtM0u9JT5e5KswIr2aJqvE\nGwx2TfKE7x0F2+Fz5rLPOZrSq/TEbvRjpawqum3zc/ZeSgky6ZXvqm20E0S4OsbshHTui5ioBlA0\nja32Kbb5sbIrfDcpLOqJJAfTBTZd9lDrwsSQ11noFr32uEkrds6K0ZuJMQLIlj5fHZq8k2llsz5N\nkp3RnoxelkAl4ZIyRjIcndtWH42eWadBlekdvZMGJ5qvFxFrwPbq4dNodCVWdWKHcKh+u53pw23o\ng5xM5hp33Tjrdi2Xf23wY/VmAz5hKmcv2PyHW31sqXGTL39NWOV9o6GNGC2w4aYysNWCiCc6fcq7\nPyqx+vOA38sqgFSmmy/XX1RmsKK2pnANJFOy1mVopFdeoYV9VAgAiKPoumYidZRYyiqdoasc8gSO\nM+ADFHFM89IYaCWTHZgUs46y5nRPYhtB7LUjV257vpTx26Z0JLWcTUXy7OsVovJ6qgXnoLXYoZJn\noSgmHg0wrFy5I8LNIb/lGFglpkGPWGXrtD2YbJm4CDrnqKgZhPk4U32Xpz1urdd2hOXVFfZviG5g\nFbdeimI47EQrcAKgYvdUPWGQPuQMVisGO+wMpSfzhlq9D+BR4lZpt8F4tz/A2Fk2wO1mvFVamrXR\nkKgHgBq9ekOOQyolYwzb7Wi9pvttrLieH72WntrwaogeLO5ZFEsh2ZVsxOCSKGnHVW3VAYcBBafl\nzf9YW9URtsFB2sgVEXa+GtUMsK1JmXMRTpgxBsG/agB0QibYJIkzTNfO+0qiauw2XEwnXVCmpxy+\n3iGJFVz+Ve7+GDhCbddbkNhPYYsXadoQYgZAIk8ADg4IJgAGJpaBJBPrpdgrgyAFvW8sPETOKqiA\nWvMiViSW4Xtc48vWvrrHlfg0PsDXJcCCWzGmJTP5l4UIfLoOWVJGxcyxc46YtEoiQofE5JyLB5vQ\nJmqvasx2VCRW2nAMzpI47k2z3nxIjx/CVwneIKiyEZW/9wwAzjlNfoRYoZRA2GOdnM45oiVJ6Zu4\n/Nk1omjKDswlVk3+7JNJXyDJROh7t08aifsxIqoJk63Pu+CFTJKfA2P2dhRWERGAT2/MzBqapx5/\nyT0OCPIJHAQYEBEJR6V4rXNyamkEadPY2rha8EP51GpWzQfz+aRKYDKRWDvb77TmtypNG5gAoRPs\nk7W1CxZaMmoVEssGp+oHmA1Ju/PSm1pzJ1otfoRE5z58abLxbAiJ8hovua0iRACAiMJCfibyuUOI\nGNEFL0N0hzvZ6Kurvz8Ri6WwObHSMjmr2j1yz9D7JcCGtxrgCNOtTxXsVE+1bpn2ZpORxi7YlL1E\nC4odhkjEwsXBmCJd/pXou8i+0DpT2WTPCbhIUlqXvZ2AAf89NyLFFBNuxYViqQ9CJRavZkM1CBKr\nJYsw/uWeVy0fb16fLuGn0V/dFFdAIIhRD4Ftlj0qTRgYEBwElYf2FKEe8D6uwRYGYhUtLDes0STZ\nP1l1KUa6N6GXZn4w/gKHLn/JgMo2b53J1uhnYr+X1wRNw/OuvlAYNcg/1Avv5wyia/rnoDFbYB59\n1WhREtXuIJrPzC2rDAvGd142EKvR6w1W2kZY53tFST8I1n8AkWC69zMaeQ+KuC6zR/5W4u0xPcyY\n9sYzzt6TltdbndKxg9Vyl1Zu7bLERvsQPYgQh80AwX1ifDWrNXxIxq/zoKZ94vSKsqvpUWK7FdOD\nGAGW/Ez1ucLqQr/Qy5m4ypr7PKxQah0z2tQsx/TGSrjbhg3NOBWFf4FTqmi8jEd0sZqc44l7iSsv\nEMBlKcg2MXEY/wEtcwzGyd2yjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x112 at 0x7A96899BC0F0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(np.uint8(xinput[index,:,:,:]), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yinput[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
